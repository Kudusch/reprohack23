<<count,echo=FALSE,message=FALSE>>=
require(readxl)
require(dplyr)

raw_data <- read_excel("ccr_overview.xlsx")

raw_data %>% mutate(type = recode(type,
                                  "?tool" = "tool",
                                  "?tools" = "tool")) %>%
    count(type) -> overview

N_PROSE <- overview$n[overview$type == "prose"]
N_MET <- overview$n[overview$type == "met"]
N_SUB <- overview$n[overview$type == "sub"]
N_TOOL <- overview$n[overview$type == "tool"]

N_EMP <- N_SUB + N_MET
cd_class <- raw_data %>% filter(type %in% c("met", "sub")) %>% count(code, data)
N_00 <- cd_class$n[1]
N_01 <- cd_class$n[2]
N_10 <- cd_class$n[3]
N_11 <- cd_class$n[4]

set.seed(721831)
raw_data %>% filter(type %in% c("met", "sub") & code == 1 & data == 1) %>% pull(id) %>% sample -> sids

.a <- function(id) {
    ## Generate the alphabetical code based on article ID 
    LETTERS[which(sids == id)]
}
@

Based on various yardsticks, the application of computational methods in communication research is in full swing. Computational communication science has earned widespread consideration as a subfield of Communication Science \parencite[]{hilbert2019computational}. The Computational Methods division of the International Communication Association (ICA CM) is one of the fastest growing divisions of the association: its membership has doubled every one to two years since ICA CM's inception. There is also a steady supply of communication papers using computational methods. According to one estimate, 2\% of papers in journalism studies journals in the last decade used this approach \parencite{zeng:2023:EMI}. Various traditional communication journals have published special issues devoted solely to computational methods (e.g. \textit{Communication Methods \& Measures} 12:2, \textit{International Journal of Communication} 13, and \textit{Political Communication} 38:1). And the most important yardstick---in our opinion---is the dedication of an official ICM CM journal: \textit{Computational Communication Research} (\textit{CCR}).

Two of the promises of computational methods for Communication scholars were that it makes research more transparent and more reproducible.
Computer code by design constitutes a detailed set of instructions on how to perform a certain task.
Hence, sharing it with other researchers makes the steps taken to arrive at results highly transparent.
Additionally, computational methods should make analyses perfectly reproducible, as the same code, running an analysis task on the same data, should always return the same results.
At the inauguration in 2019, the founding editors of \textit{CCR} commited to these promises in \textit{``A Roadmap of Computational Communication Research''} \parencite[][\textit{Roadmap I} hereinafter]{ccrintro}:

\begin{displayquote}
\textit{``CCR demands transparent and reproducible research. Computational analyses require many choices regarding design, preprocessing, and parameter tuning, and transparency are needed to allow scrutiny of these choices. As digital data and analysis code can be shared easily, computational research can be at the forefront of the open science philosophy ... Most articles in CCR should be accompanied by an online appendix in a form that encourages reproducibility and reusability.''}
\end{displayquote}

However, in practice, sharing code and data still comes with obstacles and the code to run an analysis does not only consist of the high-level code generated by the researchers, but consists of many parts:
from hardware drivers, low-level operating system features and external dependencies, such as R or Python packages and their dependencies, to randomly generated numbers for sampling and bootstrapping.
If code at any stage changes, the efforts to make it possible for others to run analyses again might have been futile or require extended additional efforts.
Computational environment can be controlled by researchers and documented for future reproducibility---if they are aware of issues, technically adapt enough, and willing to invest the effort.
If not addressed by the research community, however, we agree with \cite{mede:2020} in worrying that the looming replication crisis could erode public trust in science as a whole.

The demands for transparency and reproducibility in \textit{Roadmap I} therefore deserve our special scrutiny.
Not because we doubt the sincerity of the commitment, but because computational subfields are might be ahead of other Communication---regarding efforts to make transparency and reproducibility the norm and struggling with the hurdles that must be overcome first:
Other so-called ``Computational X'' disciplines \footnote{\url{https://writings.stephenwolfram.com/2016/09/how-to-teach-computational-thinking/}} also grapple with their own reproducibility problems \parencite[e.g.][]{hothorn:2011:C, hutson:2018:M, ioannidis:2009:R}.
Addressing remaining issues we find in the official ICA CM journal, which again is probably the most commited to transparency and reproducibility, might thus lay a more steady groundwork for the future of Communication Science and help rectify questionable research practices of the past \parencite[]{bakker:2021:QOR,matthes:2015:QRP}.
Our goal in this article is thus not to criticise the efforts already made, but to highlight which problems remain in the commendable efforts already undertaken to battle \emph{irreproducibility} of research findings.

\subsection{Transparency and reproducibility}

The first line of defense against irreproducibility, as stated in \textit{Roadmap I}, is ``an online appendix in a form that encourages reproducibility and reusability.'' At the base level, it means more transparency: Open Data and Open Materials (e.g. computer code). This call for data and code sharing can also be found in the recent calls for Open Science in communication science \parencite[]{dienlin:2020:AOS,bowman:2020:CBP,lewis:2019:OCS}.

Being transparent is a huge improvement over the opaque nature of the default scientific publication model. An article without transparency is surely not reproducible. But a paper with all the code and data made available does not equal to reproducible automatically either \parencite[]{peng:2011:RRC}. A result is reproducible, as define by \textcite{The_Turing_Way:2022} along with several other authors \parencite[]{schoch:2023:CRC,broman2017recommendations}, is ``when the
\textit{same} analysis steps performed on the \textit{same} dataset consistently produces the \textit{same} answer.'' Therefore, one must perform the same analysis on the same dataset, i.e. execute the code with the data, and check whether the same answer can be obtained consistently. Previous attempts to execute the code shared by researchers showed that most does not run \parencite[]{cruewell:2023:WB,trisovic:2022} and therefore most of the studies even with great transparency are in fact not reproducible.

In order to attempt to reproduce findings and check results under different circumstances, the shared code must be at least executable by other parties using the same data---which hence also needs to be shared.
An online appendix that can possibly enable executability was envisioned in \textit{``Toward Open Computational Communication Science: A Practical Road Map for Reusable Data and Code''} \parencite[\textit{Roadmap II} hereinafter]{van2019computational} by a similar group of authors of \textit{Roadmap I}. It is important to note that the authors of \textit{Roadmap II} emphasize \textbf{reusability} of data and code, which they define as ``allow[ing] and encourag[ing] other scholars to adapt them to their specific needs.''  \textit{Roadmap II} does not focus on making data and code executable by others \textit{per se}. However, certain ideas from it are helpful to achieve the same goal. For example, \textit{Roadmap II} encourages the use of research compendia to share fully documented data and code; the code and data should be version controlled and with unit tests; the computational environment should be preserved as Dockerfile.

\subsection{Reproducibility of computational communication science}

Half a decade has passed since \textit{Roadmap I} defined the grand vision of the subfield computational communication science as the forerunner of transparent and reproducible research and; \textit{Roadmap II} laid out the practical steps towards this vision. Still, little has been known about how successful this grand vision has been materialized.

We have some data on how (in)transparent the whole field of Communication is \parencite[]{haim:2023:H,markowitz:2021:TAE}.
However, there is no data on whether the subfield, as presented by the articles published in \textit{CCR}, is improving the picture.
%Also, there is no data on the reproducibility of findings from the subfield of computational communication science. 
These information can only be gleaned by executing the code shared by computational communication researchers.
Again, our most important goal is to document qualitatively all details that make computational communication science (ir)reproducible and to identify avenues to improve the reproduciblility of the findings from the subfield.

\section{Method}

We set the date 2023-05-25 as the ``snapshot date'' of this study. The snapshot date means this study is based on the published papers, materials (shared data and code), and technology available on this day. There are caveats to this claim (especially the last part) and we will explain these caveats in later sections. On this date or within the perimeter of a few months, the following actions were taken.

\subsection{Preregister the criteria of reproducibility failure}

On the snapshot date, we preregistered the research question (\textit{How many papers published in \textit{CCR} are not computationally reproducible?}) and protocol of the study. In the protocol, we define the following events as reproducibility failure: (1) No shared code, (2) No shared data, (3) Code execution failure, despite code rewrite, (4) Technically executable, but results with major deviations. These four Criteria are an operationalization of computational reproducibility defined by \textcite{The_Turing_Way:2022,schoch:2023:CRC,broman2017recommendations} (see above). Criterion 1 determines whether we can conduct the same analysis. Criterion 2 determines whether we can conduct the same analysis on the same data. Criterion 3 determines whether we can check whether the analysis consistently produces the same answer. Criterion 4 determines whether the answer is indeed the same.

The preregistered protocol also restricted the inclusion criteria of articles to those with results or output to be reproduced. Therefore, we included only articles making claims based on empirical analyses. Based on the preregistered protocol, a ``postmortem guide'' (see Appendix~\ref{sec:postmortem}) was authored as a guide to determine the computational reproducibility based on the artefacts generated from a code execution attempt.

\subsection{Conduct archival works}

\subsubsection{Archive all CCR articles}

All articles published in \textit{CCR} up to the snapshot date were automatically scraped from the \textit{CCR} website. In total, \Sexpr{nrow(raw_data)} articles were identified.

\subsubsection{Annotate articles and identify reproducible materials}

These \Sexpr{nrow(raw_data)} articles were annotated by three coders for the following information.

\begin{itemize}
  \item Type of article: Empirical analysis (substantive / methodological), Tool, Other (Theory paper etc.)
  \item Does the paper provide data on GitHub, OSF, or other repositories?
  \item Does the paper provide computer code on GitHub, OSF, or other repositories?
\end{itemize}

We identified \Sexpr{N_EMP} empirical papers (\Sexpr{N_SUB} substantive papers and \Sexpr{N_MET} methods papers), \Sexpr{N_TOOL} tool presentations and 4 other papers. In this analysis, $n = \Sexpr{N_EMP}$ was used as the base for the calculation of failure rate. Among these \Sexpr{N_EMP} empirical articles, only \Sexpr{N_11} articles provide code and data. Therefore, up to this point the failure rate is already \Sexpr{round((N_11 / N_EMP) * 100, 1)}\%, as \Sexpr{N_EMP - N_11} articles fulfilled Criterion 1 and/or 2 [1 only: \Sexpr{N_01}, 2 only: \Sexpr{N_10}, Both: \Sexpr{N_00}].

For the \Sexpr{N_11} papers initially coded with shared code and data, we will refer to them here as Articles \Sexpr{LETTERS[1]} to \Sexpr{LETTERS[N_11]} rather than their original titles, similar to \textcite[]{cruewell:2023:WB}. It is because our focus is not these individual articles, but how the characteristics of data and code sharing practices impact the computational reproducibility. A complete list, however, is available in the \href{https://anonymous.4open.science/r/what_makes_computational_communication_science_irreproducible}{Online Appendix} %\footnote{Disclosure: One article among these \Sexpr{N_11} articles, Article \Sexpr{.a(42)}, was authored by a coauthor of this paper. Because of that, the concerned coauthor did not involved in the reproducibility check of Article \Sexpr{.a(42)}.}
\footnote{Disclosure: One article among these \Sexpr{N_11} articles was authored by a coauthor of this paper. Because of that, the concerned coauthor did not involved in the reproducibility check of that article.}.

\subsubsection{Archive OSF materials}

Although materials on OSF are versioned, it is not possible to obtain the versioned materials programmtically. To solve this, on the snapshot date we archived all OSF materials of these \Sexpr{N_11} articles using the R package \textsc{osfr} \parencite[]{osfr}. All reproducibility checks involving OSF materials were conducted with the archived copies.

\subsection{Develop a reproducibility check pipeline}

The reproducibility check was conducted inside a Docker container based on \textsc{Rocker} \parencite{boettiger:2017:IR}, a container image based on Ubuntu Linux with preconfigured and versioned R. We added an optional layer of Python environment based on \textsc{pyenv} \parencite{pyenv}. We pinned the version of R and Python to 4.3.0 and 3.11.3 respectively, the latest version as of the snapshot date. The goal of this dockerized pipeline is to fully automate the code execution in a standardized, but broadly representative of the computational environment used by most computational communication researchers.

\subsubsection{Resolve dependencies automatically}

As most of the included articles do not provide a description of the computation environment, we automatically resolved the dependencies based on the shared code. The shared code was either from the archived OSF copies or the associated GitHub repository up to the latest commit before the snapshot date.

Shared R or Python code was scanned using \textsc{renv} \parencite[]{renv} and \textsc{pipreqs} \parencite[]{pipreqs} to identify R and Python packages used. For R packages, the system requirements (e.g. \textsc{GNU Scientific Library}) were also queried using \textsc{remotes} \parencite[]{remotes}.

Based on the scanned result, the software dependencies of the shared code were automatically installed inside the dockerized environment. We used Posit Public Package Manager \footnote{\url{https://posit.co/products/cloud/public-package-manager/}} and pinned the date to the snapshot date. By doing so, we made sure the latest versions of the R and Python packages as of the snapshot date were installed.

\subsubsection{Develop a batch execution pipeline}

The code execution part was developed as a single shell script file inside a Docker container that does the following: (1) Obtain the data and code from our OSF cache or GitHub; (2) Resolve and install dependencies automatically; (3) Optional: Code editing, using either \textsc{GNU sed} \parencite[]{pizzini2018gnu} or \textsc{GNU patch} \parencite[]{gnupatch}---to make the changes to the original material transparent reproducible; (4) Execute the code; and (5) Copy the artefacts out of the Container for postmortem analysis.

\subsection{Execute the code}

For the \Sexpr{N_11} articles, we attempted to execute the shared code in the above-mentioned Docker container. Exceptions are: 1) two Jupyter Notebooks that the original authors of Article \Sexpr{.a(21)} and Article \Sexpr{.a(22)} recommended running them on Google Colaboratory; 2) the original authors of Article \Sexpr{.a(20)} recommend using the description of their computational environment based on \textsc{packrat} \parencite[]{packrat}. We followed the recommendations accordingly.

This procedure was iterative and the preregistered protocol allows the following three actions when the code execution attempt was failed

\begin{enumerate}
    \item \textbf{Minor code rewrite}: When the execution failure was related to incorrect file paths, we edited the paths and attempted to rerun the code. We classified only the editing of file paths as minor code rewrite.
    \item \textbf{Major code rewrite}: When the execution failure was related to code quality issues i.e. bugs, we attempted to correct for the bugs and rerun. We classified this editing as major code rewrite.
    \item \textbf{Reconstruct a customized computational environment}: When the execution failure was related to software libraries and our automatic pipeline did not resolve them, we attempted to create a customized computational environment for the code to run on. 
\end{enumerate}

If all three actions cannot make the code executable, the article satisfies Criterion 3 - \textit{code execution failure, despite code rewrite}.

\subsection{Postmortem analysis}

We compared the artefacts generated from the code execution attempt with either the archived artefacts from the original repositories or from the results on the papers to look for deviations. We follow \textcite{cruewell:2023:WB} to define minor deviations as ``deviations in the decimals or obvious typographical errors.'' Deviations beyond decimals are classified as major deviations; and papers with major deviations satisfy Criterion 4.

\section{Results}

Among \Sexpr{N_11} articles we were able to evaluate, we confirmed that Articles \Sexpr{.a(22)} (with major rewrite), \Sexpr{.a(33)} (with minor rewrite) and \Sexpr{.a(6)} (with no rewrite) are essentially reproducible. Articles \Sexpr{.a(21)} (with major rewrite) and \Sexpr{.a(42)} (with minor rewrite) are largely reproducible; except parts for analyses reported in the Appendices are not executable due to missing data files. Similarly, Article \Sexpr{.a(20)} is largely reproducible; except we detected a missing file in the data extraction routine that stopped us from running it. The processed data is available. As these problems does not affect the analyses presented in the article proper, we deem Articles \Sexpr{.a(20)}, \Sexpr{.a(21)} and \Sexpr{.a(42)} as partially reproducible. Therefore, \Sexpr{R_RATE <- round((6 / N_EMP) * 100, 1); R_RATE} \% of \textit{CCR} articles are at least partially reproducible.

Among all other articles, the code execution attempts were not successful despite code editing for Articles \Sexpr{.a(14)}, \Sexpr{.a(32)}, \Sexpr{.a(34)}, \Sexpr{.a(41)}, \Sexpr{.a(43)}, and \Sexpr{.a(47)} (satisfied Criterion 3). For Article \Sexpr{.a(35)}, the code execution attempt was not successful even after we created a customized computational environment with Python 3.8 and code editing. The code and data associated with Article \Sexpr{.a(44)} (with major rewrite) are executable but the output has major deviations (satisfied Criterion 4).
Figure~\ref{fig:waffle} shows an overview of the results.

<<waffle, echo=FALSE, fig.cap="Types of (ir)reproducibility in CCR articles">>=
knitr::include_graphics("fig-waffle-1.pdf")
@

\subsection{Major issues}

\subsubsection{Incomplete sharing of data and code}

This is the leading cause of irreproducibility among these \Sexpr{N_11} articles. Although these articles were initially coded as having shared data and code, we discovered that some parts of the code or data were missing when we tried to execute the code. In many cases, we assume this was our oversight, as the repositories look complete on first glance. These incompletely shared data and code render these articles satisfying Criterion 3. As mentioned previously, the three articles deemed as partially reproducible contains incompletely shared data.

The incomplete sharing manifests in different forms: (1) sharing only example data to demonstrate the feature extraction pipeline but no actual data as well as the code for data analysis(Articles \Sexpr{.a(32)} and \Sexpr{.a(34)}); (2) Some data is shared but that cannot be used to run the provided code (Article \Sexpr{.a(43)}); (3) Missing data columns in the provided data (Article \Sexpr{.a(47)}); (4) Data is complete but the code for generating some variables is missing (Article \Sexpr{.a(14)}); (5) Materials essential for running the code (e.g. list of stopwords) is not available (Article \Sexpr{.a(47)}).

\subsubsection{Social media data antics}

A related issue to the incomplete sharing of data is the reliance on online access to social media data. When the access is no longer available, the inaccessibility manifests itself similarly to the issue above.

Article \Sexpr{.a(41)} provides over 50,000 Tweet IDs. Sharing IDs is the only permitted way of sharing data obtained from the Twitter API. In July 2023, We cannot rehydrate these Tweet IDs for free using the API provided by the X Corp. \footnote{\url{https://developer.twitter.com/en/developer-terms/more-on-restricted-use-cases}}. We can either rehydrate 10,000 IDs per month for a total of USD600 (would take 6 months); or rehydrate all IDs in one month for USD5000. Admittedly, we have neither the money nor the time.

A relative minor issue related to this is the feature extraction demos associated with Article \Sexpr{.a(21)} and Article \Sexpr{.a(22)} use \textsc{youtube-dl} to download videos from YouTube. This approach did not work in July 2023, with either the stable version of \textsc{youtube-dl} on PyPi or the development version on GitHub. 

\subsection{Minor issues}

\subsubsection{Incomplete sharing of code}

Even for the reproducible articles, some code for running minor parts in the analytical pipeline is missing. The missing parts concern with statistical tests (Article \Sexpr{.a(33)}), data visualization (Article \Sexpr{.a(6)}), and summary statistics (Article \Sexpr{.a(20)}).

\subsubsection{Code quality issues}

Our code execution attempts were in batch, e.g. using \texttt{R CMD BATCH} or \texttt{
jupyter nbconvert --execute}. It revealed that many code was developed \textbf{and checked} interactively only. It generates two issues. First, many visualizing code do not save the generated figures as files. Instead, probably reliance on a tedious and error-prone manual process of saving the figures from the development environment such as \textsc{RStudio} or \textsc{Jupyter Notebook}. Second, scripts that should be run in sequence might demand unsaved objects from the previous script to be presence in the memory (or ``workspace'') for the current script to run (Article \Sexpr{.a(44)}).

Other code quality issues that require coding editing for the code to be executable include hardcoding of absolute paths and making assumptions about certain directories are available in the current directory.

\subsubsection{Insufficient documentation}

Shared code can contain no documentation (e.g. Articles \Sexpr{.a(14)} and \Sexpr{.a(44)}). For those cases, we had to guess how to execute the code.

\subsubsection{Undocumented computational environment}

Only three articles clearly documented the computational environment used for the original analysis. This lack of documentation of the computational environment generates two layers of problems. The first layer is the operating system. The code associated with Article {\Sexpr{.a(22)}} does not contain any information on the original computational environment. We was not able to execute the code in the dockerized Linux environment successfully, until we found out that the R function \texttt{list.files} behaves differently on Windows and Linux.

The second layer is the software library. The code associated with Article {\Sexpr{.a(34)}}, for example, does not provide any information on the computational environment, e.g. which version of \textsc{pandas} was used. In the code, many deprecated functions of \textsc{pandas} were used and we needed to rewrite the code substantially to make the code executable with the current version as of the snapshot date.

\section{Discussion and conclusion}

This is so far the first study to evaluate the computational reproducibility of published communication research studies systematically. As in real life, this check offers good and bad news. We offer the bad news first: For \textit{CCR}, a journal with a higher-than-average rate of data and code sharing, \Sexpr{100 - R_RATE}\% of the empirical papers published there are not reproducible by third parties. Our check is only possible because of the high data and code sharing rate (\Sexpr{round((N_11 / N_EMP) * 100, 1)}\%). Given the fact that the average of data and code sharing is less than 5\% in other communication journals \parencite[]{haim:2023:H}, the pool of studies included is certainly not representative and our number might be an \textbf{underestimation} of the failure rate of the entire subfield of computational communication science. If we consider more communication journals, more papers would fall into Criterion 1 or 2 and boost the failure rate further. 

With \Sexpr{100 - R_RATE}\% of the empirical papers being not reproducible,  the grand vision in \textit{Roadmap I} has not been materialized. On the contrary, the subfield is on the brink to descend into a reproducibility crisis like other Computational X disciplines, if not already in a full crisis. Corrective actions must be taken. Our study also gives flesh and blood to the recent call for more Open Science for the field of communication science  \parencite[]{dienlin:2020:AOS,bowman:2020:CBP,lewis:2019:OCS}.

Speaking of which, another bad news from our check is another confirmation that being bare transparency by providing code and data is simply a precondition for computational reproducibility. It is the meaningful first step to pass Criteria 1 and 2, but still not sufficient for fostering computational reproducibility: Shared code is unlikely to be executable \parencite[]{cruewell:2023:WB,trisovic:2022}. Our check confirmed that more than half of the shared code associated with \textit{CCR} publications is not executable. The commonest reason is the incomplete sharing of data and code. Applying the software engineering term, this is a ``runtime error'', in contrast to ``compile-time error'' where we know before the code execution that the study is not reproducible, e.g. satisfying Criteria 1 and 2. ``Runtime error'' can only be found by the often tedious process of (manual) code execution. This manual check by third parties is not scalable: We (three researchers) took six months of our free time just to check the computational reproducibility of \Sexpr{N_EMP} empirical papers. For comparison, ICA CM received 201 submissions in 2021 alone. Applying the same scale, we would take more than 3 years to check one year of ICA CM submissions manually.

The good news, on the other hand, is that there are still some published \textit{CCR} papers that were found to be computationally reproducible. We can learn from these papers to improve the computational reproducibility of our works. See the \hyperref[subsec:recsres]{``Recommendations for researchers''} below. 

Before diving into those recommendations, we would like to stress an important point from our analysis: \textbf{The lack of computational reproducibility in this cohort of \textit{CCR} articles is largely \underline{not} a technological issue at all}. Instead, it is due to the lack of reproducible materials in the first place (Criteria 1 and 2). We cannot apply the so-called Technological Solutionism to ``fix'' the lack of transparency by perhaps yet another software tool to support computational reproducibility. The most effective way to save the subfield from the crisis is for the system to change. We also provide \hyperref[subsec:recssubfield]{several suggestions to the subfield} to support greater research transparency.

\subsection{Recommendations for researchers}
\label{subsec:recsres}

\subsubsection{Proactive reproducibility}

The retroactive approach to reproducibility is to make the analytical pipeline ``reproducible'' after the fact. This retroactive approach explains an often-cited reason for researchers' reluctance to share their code: time and effort to edit the code to make the code shareable \parencite[]{ch-srcscrpainp-22}.

Other than the perceived effort for editing the code, we also observed during our execution attempts that the retroactive code editing can introduces new bugs into the shared code. One emblematic minor edit we did to correct for this kind of bugs was to change the call for the data file \texttt{ccr\_data\_share.csv} in the code back to \texttt{data.csv} for Article \Sexpr{.a(21)}, where only the file \texttt{data.csv} is available.

We highly recommend that researchers should instead take a proactive approach to reproducibility, in which reproducibility is a built-in feature from the beginning. With this approach, code does not require any ---or more realistically, effortful--- editing to be shareable. Data files are tiered by whether or not they can be shared to eliminate the risk of incomplete sharing or accidentally sharing of sensitive data. Automatic reproducibility checks such as continuous integration might also be used. The computational environment should also be captured by technologies such as Docker and Apptainer.

To take this approach, the analytical pipeline must be carefully designed and communication researchers might not have the knowledge to build it on their own. One approach is for communication researchers to collaborate with research software engineers when designing the analytical pipeline. Educational resources such as The Turing Way \parencite[]{The_Turing_Way:2022} are also available.

The development of analytical pipeline with interpreted languages such as R and Python is usually interactive using tools such as the Read–Eval–Print Loop (REPL) or Jupyter Notebook. Interactive development is in fact an advantage of using interpreted languages and should be encouraged. However, we strongly recommend researchers \textbf{executing and checking} their developed code in batch (e.g. \texttt{R CMD Batch}, \texttt{Rscript}, \texttt{jupyter nbconvert --execute}). It is because interactive tools such as Jupyter Notebook do not enforce sequential execution and one can run the code in arbitrary order \parencite[]{samuel2023computational}. With batch execution, it is easier to identify simple errors such as missing files or undefined variables. Researchers can also use research compendium (next recommendation) and its associated features to ensure the sequential execution of code.

\subsubsection{Use research compendium}

\textit{Roadmap II} recommends the adoption of research compendium. The Turing Way \parencite[]{The_Turing_Way:2022} defines a research compendium as ``a collection of all digital parts of a research project including data, code, texts (protocols, reports, questionnaires, meta data). The collection is created in such a way that reproducing all results is straightforward.''

In practice, a research compendium separate documentation, code, raw data, intermediate data, and results in a reasonable folder structure. Using a research compendium, together with tools such as \textsc{here} \parencite{hererpkg} is helpful to eliminate the most common reason for the code editing we did to make code run: incorrect file path.

Two reproducible articles have their shared code and data organized as a research compendium, e.g. Articles \Sexpr{.a(22)}, \Sexpr{.a(42)}. Although not as a research compendium in a strict sense, Articles \Sexpr{.a(6)}, \Sexpr{.a(20)}, and \Sexpr{.a(21)} provide good documentation on how to reproduce the analysis and Article \Sexpr{.a(33)} separates the code and data.

Several templates of research compendium are available. Communication researchers have also developed one template \parencite[]{vanccs} and Article \Sexpr{.a(42)} uses such template (which the lead author is not from the research group developing that template). Inside the research compendium of Article \Sexpr{.a(42)}, the entire article was written in RMarkdown with \textsc{papaja} \parencite[]{papajarpkg} using the literate programming technique \parencite[mixing of prose with programming code,][]{knuth:1984}. Several of the reproducible articles (e.g. Articles \Sexpr{.a(21)}, \Sexpr{.a(33)}) also use some forms of literate programming. Literate programming eliminates the need for the error-prone copy and paste of figures and statistical tests and was recommended in \textit{Roadmap II} and by \textcite{lewis:2019:OCS}. Historically it was difficult for communication researchers to use literate programming, because communication journals usually accept Microsoft Word only (\textit{CCR} once did). The recent introduction of Quarto and \LaTeX~templates by \textit{CCR} is a boon for promoting literate programming to the subfield \footnote{\url{https://computationalcommunication.org/ccr/announcement/view/4}. This article was written in \LaTeX~on Overleaf. We used \textsc{Sweave} \parencite[]{leisch2002sweave} together with \textsc{knitr} \parencite[]{knitrrpkg} to enable literate programming}.

A good practice to ensure sequential execution of scripts is to name the scripts by their execution order, e.g. \texttt{01\_setup.R}, \texttt{02\_preprocess.R} \footnote{See this presentation by Jenny Bryan: \url{https://github.com/jennybc/how-to-name-files}}, which Articles \Sexpr{.a(20)} and \Sexpr{.a(22)} use. Another approach is to use build management tools such as \textsc{GNU make} \parencite[][Article \Sexpr{.a(42)} uses this]{stallman1988gnu} and \textsc{doit} \parencite[][which the original compendium uses]{schettino:2021}.

Finally, the computational environment can also be documented inside the research compendium. It can be really as a document (e.g. Aricle \Sexpr{.a(42)}) or better, as a Dockerfile (e.g. Article \Sexpr{.a(20)}) to enable reproducible rebuild.

\subsubsection{Reduce external dependencies}

We can dial back the clock for software version; but the same cannot be said about external dependencies. \textcite{schoch:2023:CRC} define external dependencies as ``parts in the research pipeline which some external stakeholders have complete control, but the researchers who create or use the research pipeline does not.'' Because researchers ---including the reproducibility checkers (e.g. us)--- have no control over these external dependencies, almost nothing about these external dependencies can be done when they are the main culprits of irreproducibility. For instance, we cannot dial back the clock for Twitter API to its 2021 state for Article \Sexpr{.a(41)} \parencite[see][]{assenmacher:2023:ERE}; we cannot dial back the clock also for YouTube where \textsc{youtube-dl} still works for Articles \Sexpr{.a(21)} and \Sexpr{.a(22)}. It is only luck that the external dependencies in the analytical pipelines of some articles still work (Downloading a dictionary and language models during the code execution attempts for Articles \Sexpr{.a(32)} and \Sexpr{.a(34)}).

Although not in our cohort of articles, external dependencies also manifests as data analytic APIs, e.g. Google Translate, Botometer, and ChatGPT. The practice has been criticized because of the changing algorithms at the server end \parencite[]{chan:2020:REC,rauchfleisch:2020:F,chen2023chatgpt}. Like social media APIs, they can have the same destiny: Botometer, since June 2023, is no longer available due to the closure of free Twitter API \footnote{\url{https://botometer.osome.iu.edu/}}. This serves as a wake-up call to the subfield that the reliance on external dependencies is a silent threat to computational reproducibility \parencite[]{schoch:2023:CRC,davidson2023social} and should be avoided at all cost. With the marveling of commercial LLM services such as ChatGPT in social science research, this threat becomes more imminent. \textcite[]{schoch:2023:CRC} provide several alternatives on both the data and analytical fronts. In essence: (1) researchers should explore alternative ways of obtaining data to study online communication, e.g. data donation, and; (2) use open source software and pretrained models which can be locally deployed as often as possible.

\subsection{Recommendations for the subfield}
\label{subsec:recssubfield}

\subsubsection{Doveryay, no proveryay}

The Russian proverb \textit{``doveryay, no proveryay''} (trust, but verify) was quoted by Ronald Reagan during the arms control negotiation with Mikhail Gorbachev in 1987. The same principle has been used by \textcite{willis:2020:TV} to evaluate the computational reproducibility of published scientific works. We \textbf{trust} the papers that we found to be irreproducible are probably computationally reproducible by the original authors. However, it would be much better if the reproducibility can also be \textbf{verified} by anyone.

Increasingly often, publication outlets require the reproducibility to be verified by third parties. Some publication outlets assign data editors to actively check for reproducibility of submissions \parencite[]{vilhuber2023reproducibility}. In the realm of communication, \textit{Political Communication} is the first outlet to assign a data editor \parencite[]{lawrence:2022:EN}. This practice should be promoted.

As we mentioned previously, it took a lot of energy to check for the reproducibility of published items manually and we offer two solutions. First solution is to make the data editor position funded to compensate for the time the data editor took to check for reproducibility, which \textit{Political Communication} does. Second solution is to make the reproducibility check as effortless as possible. We provide several suggestions in the ``\hyperref[subsubsec:compenviron]{Standardize the reproducible computational environment}'' section below.

\subsubsection{Incentivize data and code sharing}

Non-sharing of code and data is still the commonest reason for irreproducibility (\Sexpr{round(((N_EMP - N_11) / N_EMP) * 100, 1)}\%). As many have argued \parencite[e.g.][]{rowhani-farid:2017:W, panhuis:2014}, this is largely not a technological issue. The data and code sharing infrastructures have been very mature in many countries, especially industrialized ones. A recent survey even found a counter-intuitive relationship that scientists' higher satisfactory with the resources for data sharing is associated with a decrease in willingness for them to share data \parencite[]{borycz:2023:P}.

The willingness to share is an incentive issue \parencite[]{akdeniz:2023:S}. Sharing of data and code is not \textit{quid pro quo} for publication in many communication journals (including \textit{CCR}). Whether or not data and code sharing can impact citations remains inconclusive \parencite[]{markowitz:2021:TAE}. There is \textit{de facto} no incentive to share data and code. \textcite{schoch:2023:CRC} argue that without significant incentives sharing of data and code is mostly driven by moral responsibilities. We think in a similar term: \textit{bona fides}.

History told us reliance on \textit{bona fides} for behavioral changes does not scale up. The subfield must come up with carrots ---hopefully not sticks--- to induce researchers to share their data and code alongside their publications. We ---the authors, some junior researchers--- are not in the position to prescribe these field-level interventions for the subfield. However, communication researchers should believe in the possibility of the (sub)field to come up with a good solution to this issue, as evidenced by previous field-wise efforts such as the 70th Annual ICA Conference in promoting Open Science; and the data and software citation clause in ICA CM's Call for Papers since 2021.

We quoted \textit{``doveryay, no proveryay''} previously. As a reply to Reagan, Gorbachev quoted ``the reward of a thing well done is to have done it'' (allegedly by Ralph Waldo Emerson). It also partially answers the incentive question.

\subsubsection{Encourage protected access for sensitive data}

Another obstacle for data sharing is the sharing of sensitive data. Most \textit{CCR} articles we checked use sensitive data, such as social media data (obtained via restrictive APIs) and copy-righted data, and they are not publicly shareable. Communication researchers have come up with several solutions to this issue: Non-consumptive research \parencite{van:2020:TSY} (remote access to sensitive data without allowing users to abuse this access), sharing of encrypted data publicly \parencite[]{vanccs} are two proposals. Practically in the cohort of \textit{CCR} papers, researchers shared the processed intermediate data together with the code for data preprocessing, e.g. Articles \Sexpr{.a(20)}, \Sexpr{.a(33)}, and \Sexpr{.a(42)}.

A less technical solution is to promote ``protected access'' (or ``controlled access''), a notion that is used by the journal \textit{Psychological Science} \footnote{\url{https://www.psychologicalscience.org/publications/psychological_science/ps-submissions}}. Researchers can deposit sensitive data to approved protected access repositories (APARs) \footnote{A list of APARs is available in \url{https://osf.io/tvyxz/wiki/}. Examples are Inter-university Consortium for Political and Social Research (ICPSR) at the University of Michigan, Research Data Repository at the University of Bristol, and Datorium at GESIS – Leibniz Institute for the Social Sciences.}. These repositories take care of the access control of deposited data. For example, if one would like to reproduce the analysis of a paper using a sensitive dataset deposited in an APAR, the APAR would require a formal application for data access. Therefore, the data desposited in APARs have a better care and the APARs have a papertrail of who have a copy of the sensitive data. In other words, protected access is much more trustworthy than the so-called ``available upon request'' \parencite[]{kr-arferwssm-12}, both in terms of future access and the risk of leaking sensitive data. 

As long as someone other than the original authors (``trusted third parties'', e.g. data editor or other researchers) can have access to that sensitive data and reproduce the analysis, \textcite{schoch:2023:CRC} classify this as second-order computational reproducibility.

\subsubsection{Standardize the reproducible computational environment}
\label{subsubsec:compenviron}
Our check shows that most of the shared code and data available can run in a standardized computational environment based on an off-the-shelf Docker image \parencite[]{boettiger:2017:IR}, albeit in most of the time code editing was needed. The computational environment we used can be rebuilt reproducibily and the edited code can run inside it automatically. Our computational environment can fit majority of the use cases. It points to a possibility of having a standardized, reproducible computational environment for running and checking the code of computational communication science. 

To eliminate the need for manual code editing when checking, computational communication researchers would need to proactively develop the analytical pipeline for this standardized computational environment. If the subfield can even agree upon how the code should be executed inside this standardized computational environment (e.g. \textsc{GNU make}), that would enable automatic code execution for checking the reproducibility of computational communication research, rather than the tedious manual code execution like we (and many other) did. This automatic process can be run on anywhere Docker can run, e.g. Binder, GitHub Actions, and Heroku. The elimination of the tedious manual tasks can make the reproducibilty verification of a submitted paper immediately available to the reviewers so that it can be taken into account.

Having a standardized computational environment requires a concerted effort, or else it would descent into the ``XKCD 927'' problem \footnote{XKCD 927, How Standards proliferate: \url{https://xkcd.com/927/}}. Although we demonstrate our proposed computational environment works practically, academic groups such as ICA CM together with and a consortium of journal editors are in a better position to derive this standard.

\section{Coda: Tradeoffs}

``Programmers know the benefits of everything and the tradeoffs of nothing'', said Rich Hickey, the inventor of the Clojure language. We were probably too much into the programmer mode in the above discussion and therefore we quote Hickey here as a self-criticism. It is important to remind ourselves once again, computational reproducibility is not wholly a technological issue, in large part a social issue. When dealing with social issues, we must know tradeoffs more so than in dealing with technological issues.

The above discussion focuses only on the availability of data and code, as well as the software used for executing the code. An issue that was completely ignored in the above discussion is computational resources. For example, we took weeks to check the code of Article \Sexpr{.a(6)} on an off-the-shelf notebook computer due to the exceptionally long running time. For Article \Sexpr{.a(20)}, the demo code for video feature extraction demands a specific older generation of GPU. We managed to find one, but with the best of our knowledge we still cannot get that code run due to a missing script in the shared code. 

Our omission of discussing computational resources is intentional: we have inadequate resources. Someone might have more and therefore is a nonissue. However, computational communication science would probably become more computational intensive in the future and it renders the task of computational reproducibility check like ours more demanding. Environmentally speaking, computational power should now be considered as a scare resource due to the greenhouse gas emission associated with it. Reproducibility check like ours could be perceived as wasteful, if one sees no value in such duplicated efforts ---the \textit{re} in reproducibility.

Another omission in our discussion is that our suggestions increase the workload of computational communication researchers, whose time is already in scarcity. Getting reproducibility right is difficult, as seen in the code editing we needed to do even for articles we found to be reproducible. Although we have suggested automating this process as much as possible, there is still an upfront cost to learn about the tools. It circles back to the incentive question. Again, we have no solution.

\section{Acknowledgements}

%We would like Shelby Wilcox (Michigan State University) for her help during the conceptualization of this study; Jun Sun and Arnim Bleier (both GESIS) for giving us advice on reproducing Python computational environments. 

We would like to thank \textit{Computational Communication Science} for fostering transparent and reproducible research. We also would like to thank the original authors who submitted their works to \textit{Computational Communication Science} and shared their code and data. Without their \textit{bona fides}, this analysis of computational reproducibility is not possible.

\printbibliography

\appendix
\section{Postmortem Guide}
\label{sec:postmortem}

\includepdf[pages=-]{postmortem.pdf}
