

Based on various yardsticks, the application of computational methods in Communication Research is in full swing. Computational Communication Science has earned widespread consideration as a subfield of Communication Science \parencite[]{hilbert2019computational}. The Computational Methods division of the International Communication Association (ICA CM) is one of the fastest growing divisions of the association: its membership has doubled every one to two years since ICA CM's inception. There is also a steady supply of Communication papers using computational methods. According to one estimate, 2\% of papers in journalism studies journals in the last decade used a computational approach \parencite{zeng:2023:EMI}. Various traditional Communication journals have published special issues devoted solely to computational methods (e.g. \textit{Communication Methods \& Measures} 12:2, \textit{International Journal of Communication} 13, and \textit{Political Communication} 38:1). And the most important yardstick---in our opinion---is the dedication of an official ICM CM journal: \textit{Computational Communication Research} (\textit{CCR}).

Two of the promises of computational methods for Communication scholars are that it makes research more transparent and more reproducible.
Computer code by design constitutes a detailed set of instructions on how to perform a certain task.
Hence, sharing it with other researchers makes the steps taken to arrive at results highly transparent.
Additionally, computational methods should make analyses perfectly reproducible, as the same code, running an analysis task on the same data, should always return the same results.
At the inauguration in 2019, the founding editors of \textit{CCR} committed to these promises in \textit{``A Roadmap of Computational Communication Research''} \parencite[][\textit{Roadmap I} hereinafter, emphasis added]{ccrintro}:

\begin{displayquote}
\textit{``CCR demands transparent and reproducible research. Computational analyses require many choices regarding design, preprocessing, and parameter tuning, and transparency are needed to allow scrutiny of these choices. As digital data and analysis code can be shared easily, computational research can be at the forefront of the open science philosophy [...] Most articles in \textit{CCR} should be accompanied by an online appendix in a form that encourages reproducibility and reusability. [...] For articles presenting substantive and/or methodological analysis results and data contributions, \textbf{we expect an online research compendium published on GitHub or an equivalent service. Such a compendium contains the data, code, and results, and makes it explicit how the code is used to derive the results from the raw data.}''}
\end{displayquote}

However, in practice, sharing code and data \textbf{publicly} still comes with obstacles \footnote{It is important to reiterate that the founding editors of \textit{CCR} expect that the online research compendium containing data, code, and results to be published on \textbf{GitHub or an equivalent service}. We interpret this as a general editorial expectation for data and code to be made available publicly. } and the code to run an analysis does not only consist of the high-level code generated by the researchers, but comprises many parts:
from hardware drivers, low-level operating system features and software dependencies, such as R or Python packages and their dependencies, to randomly generated numbers for sampling and bootstrapping.
If code at any stage changes, the efforts to make it possible for others to run analyses again might have been futile or require extended additional efforts.
Technically, the computational environment can be controlled by researchers and documented for future reproducibility---if they are aware of issues, technically adapt enough, and willing to invest the effort.
If not addressed by the research community, however, we agree with \textcite{mede:2020} in worrying that the looming replication crisis could erode public trust in science as a whole.

The demands for transparency and reproducibility in \textit{Roadmap I} therefore deserve our special scrutiny.
Not because we doubt the sincerity of the commitment, but because we assume the computational subfield to be ahead of others in Communication Science regarding the efforts (and struggles) to make transparency and reproducibility the norm.
We assume that Communication Science mirrors other disciplines here, in which so-called ``Computational X'' subfields \footnote{\url{https://writings.stephenwolfram.com/2016/09/how-to-teach-computational-thinking/}} also grapple with reproducibility problems \parencite[e.g.][]{hothorn:2011:C, hutson:2018:M, ioannidis:2009:R}.
Addressing remaining issues we find today in the official ICA CM journal, which again is probably the most committed to transparency and reproducibility, might thus lay a more steady groundwork for the future of Communication Science and help rectify questionable research practices of the past \parencite[]{bakker:2021:QOR,matthes:2015:QRP}.
Our goal in this article is thus not to criticize the efforts already made, but to highlight which problems remain in the commendable efforts already undertaken to battle \emph{irreproducibility} of research findings.

This article fist presents an overview of important concepts and research connected to transparency and reproducibility.
It then describes the inclusion criteria for articles in our reproducibility analysis and how we conducted the reproducibility tests.
We discuss our results in terms of rate of reproducibility, but more importantly also detail which issues we found to make computational communication science (ir)reproducible.
Surprisingly, at least to us, was the finding that missing data was the main culprit that makes studies in \textit{CCR} fail our tests, \textbf{not any technological issue}.
We end with reflections on what individual researchers and the (sub)field could do to improve reproducibility of research to ensure the credibility of results and improve the trust in academic work.

\subsection{Transparency and Reproducibility}

The first line of defense against irreproducibility, as stated in \textit{Roadmap I}, is ``an online appendix in a form that encourages reproducibility and reusability.'' At the very least, this step would lead to more transparency: Open Data and Open Materials (e.g. computer code). This call for data and code sharing can also be found in the recent calls for Open Science in communication science \parencite[]{dienlin:2020:AOS,bowman:2020:CBP,lewis:2019:OCS}.

Transparency in scientific publications marks a significant advancement beyond the traditional opaque publication model---like a secret magic trick, research that completely lacks transparency is surely not reproducible. But a paper with all the code and data made available does not automatically become reproducible either \parencite[]{peng:2011:RRC}. As defined by \textcite{The_Turing_Way:2022} along with several other authors \parencite[i.e.,][]{schoch:2023:CRC,broman2017recommendations}, a result is reproducible ``when the
\textit{same} analysis steps performed on the \textit{same} dataset consistently produces the \textit{same} answer.''
Therefore, one must perform the same analysis on the same dataset, i.e. execute the code with the data, and check whether the same answer can be obtained consistently.
The difference between transparency and reproducibility is crucial as previous attempts to execute the code shared by researchers showed that most of it does not run without issues \parencite[]{cruewell:2023:WB,trisovic:2022}.
Hence, despite great transparency, most of these studies are, in fact, not reproducible.

In order to attempt to reproduce findings and check results under different circumstances, the shared code must at least be executable by other parties using the same data---which hence also needs to be shared.
An online appendix that can possibly enable executability was envisioned in \textit{``Toward Open Computational Communication Science: A Practical Road Map for Reusable Data and Code''} \parencite[\textit{Roadmap II} hereinafter]{van2019computational} by a similar group of authors as \textit{Roadmap I}. It is important to note that the authors of \textit{Roadmap II} emphasize \textbf{reusability} of data and code, which they define as ``allow[ing] and encourag[ing] other scholars to adapt them to their specific needs.''  \textit{Roadmap II} does not focus on making data and code executable by others \textit{per se}. However, certain ideas from it are helpful to achieve the same goal. For example, \textit{Roadmap II} encourages the use of research compendia to share fully documented data and code, that the code and data should be version controlled and with unit tests, and that the computational environment should be preserved as a Dockerfile.

\subsection{Reproducibility of Computational Communication Science}

Half a decade has passed since \textit{Roadmap I} defined the grand vision of the subfield Computational Communication Science as the forerunner of transparent and reproducible research and \textit{Roadmap II} laid out the practical steps towards this vision. Still, little is known about how successfully this grand vision was realized.

While we have some data on how (in)transparent the whole field of Communication is \parencite[]{haim:2023:H,markowitz:2021:TAE,Knoepfle2024}, there is no data on whether the subfield, as presented by the articles published in \textit{CCR}, is improving the picture.
This information can only be gleaned by executing the code shared by computational communication researchers---which we attempt in this study.
Again, our most important goal is to qualitatively document all details that make computational communication science (ir)reproducible and to identify avenues to improve the reproducibility of the findings from the subfield.

\section{Data and Approach}

We attempt to reproduce all studies published in \textit{CCR}. We set the date 2023-05-25 as the ``snapshot date'' of this study. The snapshot date means this current study is based on the published papers, materials (shared data and code), and technology available on this day. There are caveats to this claim (especially the last part) and we will explain these caveats in later sections. On this date or within the perimeter of a few months, the following actions were taken.

% \subsection{Preregister the criteria of reproducibility failure}

On the snapshot date, we preregistered the research question (\textit{How many papers published in \textit{CCR} are not computationally reproducible?}) \footnote{Although our research question was preregistered as such (which we cannot change), it is our intention to study this question from the third-party perspective, i.e. \textit{How many papers published in \textit{CCR} are not computationally reproducible \textbf{by third parties, e.g. us?}}. The original preregistration is available here: \url{https://doi.org/10.17605/OSF.IO/EJCSK}.} and protocol of the study.

In the protocol, we define the following events as failures to reproduce the analyses: (1) No shared code, (2) No shared data, (3) Code execution failure, despite code rewrite, (4) Technically executable, but results with major deviations. These four criteria are an operationalization of computational reproducibility defined by \textcite{The_Turing_Way:2022,schoch:2023:CRC,broman2017recommendations} (see above). Criterion 1 determines whether we can conduct the same analysis. Criterion 2 determines whether we can conduct the same analysis on the same data. Criterion 3 determines whether we can check if the analysis consistently produces the same answer. Criterion 4 determines whether the answer is indeed the same.

Our preregistered protocol established specific inclusion criteria for articles, limiting selection to those with results or outputs that can potentially be reproduced. 
Consequently, we only included articles that presented claims grounded in empirical analyses.
Based on the preregistered protocol, a ``postmortem guide'' (see Appendix~\ref{sec:postmortem}) was authored as a guide to determine the computational reproducibility using the artifacts generated from a code execution attempt.

\subsection{Data}%Conduct archival works}

%\subsubsection{Archive all CCR articles}

All articles published in \textit{CCR} up to the snapshot date were automatically scraped from the \textit{CCR} website. In total, 47 articles were identified.
%\subsubsection{Annotate articles and identify reproducible materials}
These 47 articles were annotated by three coders for the following information:

\begin{itemize}
  \item Type of article: Empirical analysis (substantive / methodological), Tool, Other (Theory paper etc.)
  \item Does the paper provide data on GitHub, OSF, or other repositories?
  \item Does the paper provide computer code on GitHub, OSF, or other repositories?
\end{itemize}

We identified 30 empirical papers (21 substantive papers and 9 methods papers), 13 tool presentations and 4 other papers. Among these 30 empirical articles, only 14 articles provide code and data. Therefore, up to this point we are not able to reproduce the findings from 46.7\% of CCR papers, as 16 articles fulfilled Criterion 1 and/or 2 [1 only: 1, 2 only: 3, both: 12].

For the 14 papers initially coded with shared code and data, we will refer to them here as Articles A to N rather than their original titles, similar to \textcite[]{cruewell:2023:WB}. This is because our focus is not these individual articles, but how the characteristics of data and code sharing practices impact the computational reproducibility. A complete list, however, is available in the \href{https://github.com/Kudusch/reprohack23}{Online Appendix} %\footnote{Disclosure: One article among these 14 articles, Article N, was authored by a coauthor of this paper. Because of that, the concerned coauthor was not involved in the reproducibility check of Article N.}
\footnote{Disclosure: One article in the set %among these 14 articles
was authored by a coauthor of this paper. Because of that, the concerned coauthor was not involved in the reproducibility check of that article.}.

%\subsubsection{Archive OSF materials}

Although materials on OSF are versioned, it is not possible to obtain the versioned materials programmatically. To solve this, on the snapshot date we archived all OSF materials of these 14 articles using the R package \textsc{osfr} \parencite[]{osfr}. All reproducibility checks involving OSF materials were conducted with the archived copies.

\subsection{Our Reproducibility Check Pipeline}

We devise a reproducibility check pipeline for executing and documenting the reproduction of the included articles (see figure~\ref{fig:pipeline}).
This sections highlights our choices regarding the computational environment; how we resolved dependencies; how we executed the code; and how we checked the outcome.

\begin{figure}
\includegraphics[width=10cm]{reproduction_pipeline_vertical.pdf}
\caption{Reproducibility Check Pipeline}
\label{fig:pipeline}
\end{figure}

The reproducibility check was conducted inside a Docker container based on \textsc{Rocker} \parencite{boettiger:2017:IR}, a container image based on Ubuntu Linux with preconfigured and versioned R. We added an optional layer of Python environment based on \textsc{pyenv} \parencite{pyenv}. We pinned the version of R and Python to 4.3.0 and 3.11.3 respectively, the latest version as of the snapshot date. The goal of this dockerized pipeline is to fully automate the code execution in a standardized computational environment, that is broadly representative of what is used by computational communication researchers.

%\subsubsection{Resolve dependencies automatically}

As most of the included articles do not provide a description of the computation environment, we automatically \textbf{resolved the dependencies} based on the shared code. The shared code was either from the archived OSF copies or the associated GitHub repository up to the latest commit before the snapshot date.

Shared R or Python code was scanned using \textsc{renv} \parencite[]{renv} and \textsc{pipreqs} \parencite[]{pipreqs} to identify the employed R and Python packages. For R packages, the system requirements (e.g. \textsc{GNU Scientific Library}) were also queried using \textsc{remotes} \parencite[]{remotes}.

Based on the scanned result, the software dependencies of the shared code were automatically installed inside the dockerized environment. We used Posit Public Package Manager \footnote{\url{https://posit.co/products/cloud/public-package-manager/}} and pinned the date to the snapshot date. By doing so, we made sure the latest versions of the R and Python packages as of the snapshot date were installed.

%\subsubsection{Develop a batch execution pipeline}

The \textbf{code execution} part was developed as a single shell script file inside a Docker container that does the following: (1) Obtain the data and code from our OSF cache or GitHub; (2) Resolve and install dependencies automatically; (3) Optional: Code editing, using either \textsc{GNU sed} \parencite[]{pizzini2018gnu} or \textsc{GNU patch} \parencite[]{gnupatch}---to make the changes to the original material transparent and reproducible; (4) Execute the code; and (5) Copy the artifacts out of the Container for postmortem analysis (see Appendix~\ref{sec:postmortem}).

%\subsection{Execute the code}

For the 14 articles, we attempted to \textbf{execute the shared code} in the above-mentioned Docker container. Exceptions are: 1) two Jupyter notebooks, which the authors of article B and article H recommend to run on Google Colaboratory; 2) Article J, for which the authors have prepared a description of their computational environment based on \textsc{packrat} \parencite[]{packrat}. We followed the recommendations accordingly.

This procedure was iterative and the preregistered protocol allows the following three actions when the code execution attempt was not successful:

\begin{enumerate}
    \item \textbf{Minor code rewrite}: When the execution failure was related to incorrect file paths, we edited the paths and attempted to rerun the code. We classified only the editing of file paths as minor code rewrite.
    \item \textbf{Major code rewrite}: When the execution failure was related to code quality issues, i.e. bugs, we attempted to correct for the bugs and rerun. We classified this editing as major code rewrite.
    \item \textbf{Reconstruct a customized computational environment}: When the execution failure was related to software libraries and our automatic pipeline did not resolve them, we attempted to create a customized computational environment for the code to run on. 
\end{enumerate}

If all three actions cannot make the code executable, the article satisfies Criterion 3 -- \textit{code execution failure, despite code rewrite}.

%\subsection{Postmortem analysis}

After code execution, we conducted a \textbf{postmortem analysis} based on the aforementioned postmortem guide (Appendix~\ref{sec:postmortem}):
We compared the artifacts generated from the code execution attempt with either the archived artifacts from the original repositories or from the results on the papers to look for deviations. We follow \textcite{cruewell:2023:WB} to define minor deviations as ``deviations in the decimals or obvious typographical errors.'' Deviations beyond decimals are classified as major deviations. We follow also \textcite{cruewell:2023:WB} to use the distinction between minor and major deviations as the cut-off point \footnote{\textcite{cruewell:2023:WB} use three categories: ``exactly reproducible'' (no deviations), ``essentially reproducible'' (with minor deviations), and ``largely not reproducible'' (with major deviations). For simplicity, we merged ``exactly reproducible'' with ``essentially reproducible'' into one category. Although it did not state in the original definition, we understood that the minor deviations in the decimals should not affect the conclusion. For example, rounding of p-value from 0.32 to 0.4 would be a minor deviation that does not affect the main conclusion. However, for precise measurements, e.g. effect sizes, a change from 0.32 to 0.4 might change the conclusion substantially.}, i.e. papers with major deviations satisfy Criterion 4. The reasons for a paper satisfying Criterion 4 can either be that the code itself cannot consistently produce results within a small window of deviations in decimals; or that the code can produce consistent results, but there were human errors in reporting.

\section{Results}
\subsection{Quantitative Results}

In this section, we present our findings in terms of how many of the attempts we consider a success or failure in reproducing the selected articles, followed by our qualitative assessment about both major and minor issues uncovered during our analysis in the next sections.
Among the 14 articles we were able to evaluate, we confirmed that Articles H (with major code rewrite), E (with minor code rewrite) and M (with no code rewrite) are essentially reproducible. Articles B (with major code rewrite) and N (with minor code rewrite) are largely reproducible, except parts of the analyses reported in the Appendices, for which some data files were missing. Similarly, Article J is largely reproducible; except we detected a missing file in the feature extraction demo that stopped us from running it (the processed data is available). As these problems do not affect the analyses presented in the main body of the article, we deem Articles J, B and N partially reproducible \footnote{We did not preregister what to do with the code for analyses not in the main body of an article, i.e. supplementary analyses in online appendices. Our decision was to run the code beyond the main analysis anyway (if it is available). We made an \textit{ad hoc} decision that irreproducibility in the supplementary analyses does not satisfy Criteria 3 and 4 but report it here. This can be considered a deviation from the preregistration. As \textcite{cruewell:2023:WB} consider main analyses only, our decision should be at least principled. However, given the field difference and supplementary analyses are common in our subfield, further studies should consider how to deal with (ir)reproducibility beyond the main analysis.}. Therefore, 20 \% of articles in \textit{CCR} that made claims based on empirical analysis and 42.9 \% of the articles that shared code and data are at least partially reproducible.

Among all other articles, the code execution attempts were not successful despite code editing for Articles D, L, F, K, A, and I (satisfied Criterion 3). In case of Article C, we think that it might be possible to eventually reproduce it. Yet after rewriting individual files and creating a customized computational environment, as some required packages can only be installed with Python 3.8, we conceded when it became clear that we do not understand in which order to run the many different steps included in the sophisticated analysis pipeline. The code and data associated with Article G (with major rewrite) are executable but the output has major deviations (satisfied Criterion 4) \footnote{We cannot confirm the actual reason for this. But it is very unlikely to be human errors in reporting because the inconsistent results are in the figures generated by code. Instead, we have reasons to believe that the code  of Article G for data visualization is incomplete.}.

Figure~\ref{fig:waffle} shows an overview of the results within the set of all 47 published \textit{CCR} articles.
Below we summarize the major and minor issues that kept us from reproducing the tested articles.

\begin{figure}
\includegraphics[width=10cm]{fig-waffle-1.pdf}
\caption{Types of (ir)reproducibility in \textit{CCR} articles.}
\label{fig:waffle}
\end{figure}

%<<waffle, echo=FALSE, fig.cap="Types of (ir)reproducibility in CCR articles">>=
%knitr::include_graphics("fig-waffle-1.pdf")
%@

\subsection{Major Issues}

\subsubsection{Incomplete Sharing of Data and Code}

Incomplete Sharing of Data and Code is the leading cause of irreproducibility among these 14 articles---which comes on top of the 16 that completely failed to share data and/or code. While these articles were initially categorized as having shared both data and code, upon attempting code execution, we found that certain elements of the code or data were, in fact, missing. These omissions seem like simple oversights, as the repositories appear to be comprehensive. It was only through our reproduction attempts that these small, yet consequential gaps came to light. The incompletely shared data and code render these articles satisfying Criterion 3 (code execution failure, despite code rewrite). As mentioned previously, even the three articles deemed partially reproducible base some of their analyses on data not included in the shared material.

The incomplete sharing manifests in different forms: (1) sharing only example data to demonstrate the feature extraction pipeline but no actual data as well as the code for data analysis (Articles L and F); (2) Some data is shared but that cannot be used to run the provided code (Article A); (3) Missing data columns in the provided data (Article I); (4) Data is complete but the code for generating some variables is missing (Article D); (5) Materials essential for running the code (e.g. list of stopwords) is not available (Article I).

\subsubsection{Social Media Data Antics}

A related issue to the incomplete sharing of data is the reliance on online access to social media data. When the access is no longer available, the inaccessibility manifests itself similarly to the issue above.

Article K provides over 50,000 Tweet IDs. Sharing IDs is the only permitted way of sharing data obtained from the Twitter API. In July 2023, we cannot rehydrate, i.e. retrieve the Tweets' complete information using their IDs, for free using the API provided by X Corp. \footnote{\url{https://web.archive.org/web/20230728143607/https://developer.twitter.com/en/developer-terms/more-on-restricted-use-cases}}. We can either rehydrate 10,000 IDs per month for a total of USD600 (which would take 6 months); or rehydrate all IDs in one month for USD5000. Admittedly, we have neither the money nor the time.

A relatively minor issue related to this is that the feature extraction demos associated with Article B and Article H use \textsc{youtube-dl} to download videos from YouTube. This approach did not work in July 2023, with either the stable version of \textsc{youtube-dl} on PyPi or the development version on GitHub.
Despite not being illegal, as some use cases, including academic research of publicly available videos, constitute fair use \parencite[]{HennesyLaw2019}, YouTube and other companies continue to fight \textsc{youtube-dl} with legal and technical means to hinder its usage.
As already pointed out by \textcite{freelonPostAPI2018}, availability of social media data is up to the whims of the companies who provide access to it and will probably further decrease in the future.

\subsection{Minor Issues}

\subsubsection{Incomplete Sharing of Code}

Even for the reproducible articles, some code for running minor parts in the analytical pipeline is missing. The missing parts concern with statistical tests (Article E), data visualization (Article M), and summary statistics (Article J).

\subsubsection{Code Quality Issues}

Our code execution attempts were in batch, e.g. using \texttt{R CMD BATCH} or \texttt{
jupyter nbconvert --execute}. Some of the issue we encountered are likely because the code in question was developed \textbf{and checked} interactively only. This generates two issues. First, most visualization functions do not save the generated figures as files. Therefore, many researchers seem to rely on a tedious and error-prone manual process of saving the figures from the development environment such as \textsc{RStudio} or \textsc{Jupyter Notebook}. Second, scripts that should be run in sequence might demand unsaved objects from the previous script to be present in the memory (or ``workspace'') for the current script to run (Article G).
Another code quality issue we encountered and which required code editing, is the use of hardcoded absolute paths and other assumptions about the availability and structure of certain directories and files.

\subsubsection{Insufficient Documentation}

Some code did not contain any documentation (e.g. Articles D and G). For those cases, we had to guess how to execute the code. Mostly, this was relatively simple, although, as explained above, Article C contains a rather sophisticated analysis pipeline, which we were not able to retrace, turning the lack of documentation from a minor into a major issue in that case.

\subsubsection{Undocumented Computational Environment}

Only three articles clearly documented the computational environment used for the original analysis. This lack of documentation of the computational environment generates two layers of problems. The first layer is the operating system. The code associated with Article {H} does not contain any information on the original computational environment. We were not able to execute the code in the dockerized Linux environment successfully, until we found out that the R function \texttt{list.files} behaves differently on Windows and Linux.

The second layer is the software library. The code associated with Article {F}, for example, does not provide any information on the computational environment, e.g. which version of \textsc{pandas} was used. In the code, many deprecated functions of \textsc{pandas} were used and we needed to rewrite the code substantially to make it executable with the current version as of the snapshot date.

\section{Discussion and Conclusion}

This is so far the first study to evaluate the computational reproducibility of published communication research studies systematically. As is often the case in life, we derive good and bad news from our scrutiny of the reproducibility of articles published in \textit{CCR}. We offer the bad news first: For \textit{CCR}, a journal with a higher-than-average rate of data and code sharing, 80\% of all empirical papers and 57.1 \% of the articles that shared code and data are not reproducible by third parties. However, our check is only possible because of the high data and code sharing rate (46.7\%). Given the fact that the average of data and code sharing is less than 5\% in other communication journals \parencite[]{haim:2023:H,Knoepfle2024}, the pool of studies included here is certainly not representative and the share of irreproducible articles we report might be an \textbf{underestimation} of the actual number for the entire subfield of computational communication science. If we consider more communication journals, more papers would fall into Criterion 1 or 2 and a higher proportion of papers would be computationally irreproducible by third parties. 

With 80\% of the empirical papers being not reproducible by third parties (57.1 \% were code and data was shared), the grand vision in \textit{Roadmap I} has not (yet) been realized. On the contrary, the subfield is on the brink to descend into a reproducibility crisis like other Computational X disciplines, if not already in a full crisis. Corrective actions must be taken. Our study also gives flesh and blood to the recent calls for more Open Science for the field of communication science  \parencite[]{dienlin:2020:AOS,bowman:2020:CBP,lewis:2019:OCS}.

On that note, more bad news we derive from our check is a confirmation that simply providing code and data---what we might term 'basic transparency'---is just a starting point and not a guarantee of computational reproducibility. It is the meaningful first step to pass Criteria 1 and 2, but still not sufficient for fostering computational reproducibility: As \textcite{cruewell:2023:WB,trisovic:2022} have noted before, most shared code is not executable without further actions. Our check confirmed that more than half of the shared code associated with \textit{CCR} publications is not executable even with code rewrite. The most common is the incomplete sharing of data and code. Applying the software engineering term, this is a ``runtime error'', in contrast to ``compile-time error'' where we know before the code execution that the study is not reproducible, e.g. satisfying Criteria 1 and 2. ``Runtime error'' can only be found by the often tedious process of (manual) code execution. This manual check by third parties is not scalable: We (three researchers) took six months of our time just to check the computational reproducibility of 30 empirical papers. For comparison, ICA CM received 201 submissions in 2021 alone. Applying the same scale, we would take more than 3 years to check one year of ICA CM submissions manually.

The good news, on the other hand, is that there are still some published \textit{CCR} papers that were found to be computationally reproducible. We can learn from these papers to improve the computational reproducibility of our works. See the \hyperref[subsec:recsres]{``Recommendations for researchers''} below. 

Before diving into those recommendations, we would like to stress an important point from our analysis: \textbf{For most \textit{CCR} articles, our code execution attempts were \underline{not} blocked by technological issues}. Instead, it is due to the lack of reproducible materials in the first place (Criteria 1 and 2). The technical recommendations we give might take up most of the room in this part of our article, yet this is mostly a function of these tips being relatively straightforward to implement by junior researchers---such as ourselves. The field as a whole must not fall into the trap of so-called Technological Solutionism to ``fix'' the lack of transparency by perhaps yet another software tool to support computational reproducibility. The most effective way to save the subfield from the crisis is for the incentive system to change. We also provide \hyperref[subsec:recssubfield]{several suggestions to the subfield} to support greater research transparency, which we hope journal editors and senior researchers in the field will focus on.

\subsection{Recommendations for Researchers}
\label{subsec:recsres}

\subsubsection{Execute and Check Code in Batch}

The development of analytical pipeline with interpreted languages such as R and Python is usually interactive using tools such as the Read–Eval–Print Loop (REPL) or Jupyter Notebook. Interactive development is in fact an advantage of using interpreted languages and should be encouraged. However, we strongly recommend researchers \textbf{executing and checking} their developed code in batch (e.g. \texttt{R CMD Batch}, \texttt{Rscript}, \texttt{jupyter nbconvert --execute}, or \texttt{quarto render}). This is because interactive tools such as Jupyter Notebook do not enforce sequential execution and one can run the code in arbitrary order \parencite[]{samuel2023computational}. With batch execution, it is easier to identify simple errors such as missing files or undefined variables. Ideally, researchers design their project with the goal in mind that the entire analysis pipeline can be re-created by running scripts for data gathering, processing, analysis and summarizing (e.g., in plots and tables) in order. Researchers can also use a research compendium (next recommendation) and its associated features to ensure the sequential execution of code.

\subsubsection{Use Research Compendium}

Both \textit{Roadmap I} and \textit{Roadmap II} recommend the adoption of research compendia. The Turing Way \parencite[]{The_Turing_Way:2022} defines a research compendium as ``a collection of all digital parts of a research project including data, code, texts (protocols, reports, questionnaires, meta data). The collection is created in such a way that reproducing all results is straightforward.''

In practice, a research compendium separates documentation, code, raw data, intermediate data, and results in a reasonable folder structure. Using a research compendium, together with tools such as \textsc{here} \parencite{hererpkg}, or by adopting relative paths in RMarkdown/Quarto documents, can eliminate the most common reason for the code editing we did to make code run: incorrect file paths.

Two reproducible articles have their shared code and data organized as a research compendium: Articles H and N. Although not as a research compendium in a strict sense, Articles M, J, and B provide good documentation on how to reproduce the analysis and Article E separates code and data.

Several research compendium templates are already available, one of which developed by Communication researchers \parencite[]{vanccs}. Article N uses this template, which made the reproduction attempt considerable more straightforward. Inside the research compendium of Article N, the entire article was written in RMarkdown with \textsc{papaja} \parencite[]{papajarpkg} using the literate programming technique \parencite[mixing of prose with programming code,][]{knuth:1984}. Several of the reproducible articles (e.g. Articles B, E) also use some forms of literate programming. Literate programming eliminates the need for the error-prone copy and paste of figures and statistical tests and was recommended in \textit{Roadmap II} and by \textcite{lewis:2019:OCS}. Historically it was difficult for communication researchers to use literate programming, because communication journals usually accept Microsoft Word only (\textit{CCR} once did). The recent introduction of Quarto and \LaTeX~templates by \textit{CCR} is a boon for promoting literate programming to the subfield \footnote{\url{https://computationalcommunication.org/ccr/announcement/view/4}. This article was written in \LaTeX~on Overleaf. We used \textsc{Sweave} \parencite[]{leisch2002sweave} together with \textsc{knitr} \parencite[]{knitrrpkg} to enable literate programming}.

A good practice to ensure sequential execution of scripts is to name the scripts by their execution order, e.g. \texttt{01\_setup.R}, \texttt{02\_preprocess.R} \footnote{See this presentation by Jenny Bryan: \url{https://github.com/jennybc/how-to-name-files}}, which Articles J and H use. Another approach is to use build management tools such as \textsc{GNU make} \parencite[][Article N uses this]{stallman1988gnu} and \textsc{doit} \parencite[][which the original compendium uses]{schettino:2021}.

Finally, the computational environment can also be documented inside the research compendium. This can be done either as a document listing all dependencies and their versions (e.g. Aricle N) or better, as a Dockerfile (e.g. Article J) to enable reproducible rebuild.

\subsubsection{Reduce External Dependencies}

We can dial back the clock for software versions; but the same cannot be said about external dependencies. \textcite{schoch:2023:CRC} define external dependencies as parts in the research pipeline that some external entities have complete control, but researchers using those parts have not. Because researchers---including the reproducibility checkers (e.g. us)---have no control over these external dependencies, almost nothing about these external dependencies can be done when they are the main culprits of irreproducibility. For instance, we cannot dial back the clock for Twitter's API to its 2021 state for Article K \parencite[see][]{assenmacher:2023:ERE}; neither can we dial back the clock for YouTube where \textsc{youtube-dl} still works for Articles B and H. It is only luck that the external dependencies in the analytical pipelines of some articles still work (Downloading a dictionary and language models during the code execution attempts for Articles L and F). Relying on external dependencies for data collection might be unavoidable, depending on the goal of the research. But they should be avoided where possible when offering reproduction materials \parencite[e.g.,][]{nonconsumptive,schoch:2023:CRC}.

Although not in our cohort of articles, external dependencies also manifest as data analytic APIs, e.g., Google Translate, Botometer, and  Microsoft's Face API. The practice has been criticized because of the changing algorithms at the server end \parencite[]{chan:2020:REC,rauchfleisch:2020:F,chen2023chatgpt}. Like social media APIs, they can have the same destiny: Botometer, since June 2023, is no longer available due to the closure of the free Twitter API \footnote{\url{https://botometer.osome.iu.edu/}}. This serves as a wake-up call to the subfield that the reliance on external dependencies is a silent threat to computational reproducibility \parencite[]{schoch:2023:CRC,davidson2023social} and should be avoided where possible. For example, the offering of OpenAI (a profit-seeking external entity, like X Corp.) to make the best state-of-the-art large language models (LLMs) available as an affordable commercial service via their API might be tempting for social science researchers. However, we need to recognize that they do not provide the same route to long-term reproducibility that open source LLMs offer. As these models often perform similarly \parencite[]{weber:2024:E}, the field should consider trading a little performance for a significant improvement in reproducibility to evade a new generation of irreproducible research when OpenAI retires older models or makes them unaffordable through price increases. Rather than relying on external dependencies simply for the reason that they offer the best performance, the field should take into consideration which approach offers the best performance-reproducibility ratio. Which tips the scale in favor of open source software and models that can be deployed in an environment controlled by the researcher (e.g., locally) \parencite[]{schoch:2023:CRC}.


\subsubsection{Proactive Reproducibility}

The retroactive approach to reproducibility is to make the analytical pipeline ``reproducible'' after the fact. This retroactive approach explains an often-cited reason for researchers' reluctance to share their code: time and effort to edit the code to make it shareable \parencite[]{ch-srcscrpainp-22}.

Other than the perceived effort for editing the code, we also observed during our execution attempts that the retroactive code editing can introduce new bugs into the shared code. One emblematic minor edit we did to correct for this kind of bugs was to change the call for the data file \texttt{ccr\_data\_share.csv} in the code back to \texttt{data.csv} for Article B, where only the file \texttt{data.csv} is available.

We highly recommend that researchers should instead take a proactive approach to reproducibility, in which reproducibility is a built-in feature from the beginning. With this approach, code does not require any---or more realistically, any effortful---editing to be shareable. Data files are tiered by whether or not they can be shared to eliminate the risk of incomplete sharing or accidentally sharing of sensitive data. Automatic reproducibility checks such as continuous integration might also be used. The computational environment should also be captured by technologies such as Docker and Apptainer.

To take this approach, the analytical pipeline must be carefully designed and communication researchers might not have the knowledge to build it on their own. One approach is for communication researchers to collaborate with research software engineers when designing the analytical pipeline. Educational resources such as The Turing Way \parencite[]{The_Turing_Way:2022} are also available.

\subsection{Recommendations for the Subfield}
\label{subsec:recssubfield}

\subsubsection{доверяй, но проверяй}

The Russian proverb доверяй, но проверяй (trust, but verify) was quoted by Ronald Reagan during the arms control negotiation with Mikhail Gorbachev in 1987. The same principle has been used by \textcite{willis:2020:TV} to evaluate the computational reproducibility of published scientific works. We \textbf{trust} the papers that we found to be irreproducible are probably computationally reproducible by the original authors on the machine they conducted the original analysis on. However, the goal should be that reproducibility can also be \textbf{verified} by anyone, everywhere, at any time. There is a need to move beyond the so-called ``first-order computational reproducibility'' \parencite{schoch:2023:CRC} or ``repeatability'' \parencite{mcarthur:2019:RRR}, which cannot be independently verified.

Increasingly often, publication outlets require the reproducibility to be verified by third parties. Some publication outlets assign data editors to actively check for reproducibility of submissions \parencite[]{vilhuber2023reproducibility}. In the realm of communication science, \textit{Political Communication} is the first outlet to assign a data editor \parencite[]{lawrence:2022:EN}. This practice should be promoted.

As we mentioned previously, it took a lot of energy to check for the reproducibility of published items manually and we offer two solutions. The first solution is to make the data editor position funded to compensate for the time the data editor took to check for reproducibility, which \textit{Political Communication} does. The second solution is to make the reproducibility check as effortless as possible. We provide several suggestions in the ``\hyperref[subsubsec:compenviron]{Standardize the reproducible computational environment}'' section below.

\subsubsection{Incentivize Data and Code Sharing}

Non-sharing of code and data is still the most common reason for irreproducibility (53.3\%). As many have argued \parencite[e.g.][]{rowhani-farid:2017:W, panhuis:2014}, this is largely not a technological issue. The data and code sharing infrastructures have been very mature in many countries, especially industrialized ones. In fact, a recent survey found the counterintuitive relationship that higher scientist satisfaction with data sharing resources is associated with a lower willingness to share data \parencite[]{borycz:2023:P}.

The willingness to share is an incentive issue \parencite[]{akdeniz:2023:S}. Sharing of data and code is not required for publication in many communication journals (including \textit{CCR}). Whether or not data and code sharing can impact citations in communication science remains inconclusive \parencite[]{markowitz:2021:TAE}. There is \textit{de facto} no incentive to share data and code. \textcite{schoch:2023:CRC} argue that without significant incentives, sharing of data and code is mostly driven by moral responsibilities. %We think in a similar term: \textit{bona fides}.

However, history told us that reliance on \textit{bona fides} for behavioral changes does not scale up. The subfield must come up with carrots---hopefully not sticks---to induce researchers to share their data and code alongside their publications. We---the authors, some junior researchers---are not in the position to prescribe these field-level interventions for the subfield. However, communication researchers should believe in the possibility of the (sub)field to come up with a good solution to this issue, as evidenced by previous field-wise efforts such as the 70th Annual ICA Conference in promoting Open Science; and the data and software citation clause in ICA CM's Call for Papers since 2021.

We quoted доверяй, но проверяй previously. As a reply to Reagan, Gorbachev quoted ``the reward of a thing well done is to have done it'' (allegedly by Ralph Waldo Emerson). It also partially answers the incentive question.

\subsubsection{Encourage Protected Access for Sensitive Data}

Another obstacle for data sharing is the sharing of sensitive data. Most \textit{CCR} articles we checked use sensitive data, such as social media data (obtained via restrictive APIs) and copy-righted data, and they are not publicly shareable. Communication researchers have come up with several solutions to this issue: Non-consumptive research \parencite{van:2020:TSY, nonconsumptive} (providing access to data analysis capabilities of data without granting access to the often sensitive data itself) and sharing of encrypted data publicly \parencite[]{vanccs} are two proposals. Practically in the cohort of \textit{CCR} papers, researchers shared the processed intermediate data together with the code for data preprocessing, e.g. Articles J, E, and N.

A less technical solution is to promote ``protected access'' (or ``controlled access''), a notion that is used by the journal \textit{Psychological Science} \footnote{\url{https://www.psychologicalscience.org/publications/psychological_science/ps-submissions}}. Researchers can deposit sensitive data to approved protected access repositories (APARs) \footnote{A list of APARs is available in \url{https://osf.io/tvyxz/wiki/}. Examples are Inter-university Consortium for Political and Social Research (ICPSR) at the University of Michigan, Research Data Repository at the University of Bristol, and Datorium at GESIS – Leibniz Institute for the Social Sciences.}. These repositories take care of the access control of deposited data. For example, if one would like to reproduce the analysis of a paper using a sensitive dataset deposited in an APAR, the APAR would require a formal application for data access. Therefore, the data deposited in APARs have a better care and the APARs have a paper-trail of who has a copy of the sensitive data. In other words, protected access is much more trustworthy than the so-called ``available upon request'' \parencite[]{kr-arferwssm-12}, both in terms of future access and the risk of leaking sensitive data. 

As long as someone other than the original authors (``trusted third parties'', e.g. data editor or other researchers) can have access to that sensitive data and reproduce the analysis, \textcite{schoch:2023:CRC} classify this as second-order computational reproducibility.

\subsubsection{Standardize the Reproducible Computational Environment}
\label{subsubsec:compenviron}
Our check shows that most of the shared code and data available can run in a standardized computational environment based on an off-the-shelf Docker image \parencite[]{boettiger:2017:IR}, albeit most of the time code editing was needed. The computational environment we used can be rebuilt reproducibily and the edited code can run inside it automatically. Hence, our computational environment should fit a majority of the use cases. It points to a possibility of having a standardized, reproducible computational environment for running and checking the code of computational communication science. 

To eliminate the need for manual code editing when checking, computational communication researchers would need to proactively develop the analytical pipeline for this standardized computational environment. If the subfield can even agree upon how the code should be executed inside this standardized computational environment (e.g. \textsc{GNU make}), that would enable automatic code execution for checking the reproducibility of computational communication research, rather than the tedious manual code execution like we (and many other) did. This automatic process can be run anywhere Docker can run, e.g. Binder, GitHub Actions, and Heroku. The elimination of the tedious manual tasks can make the reproducibility verification of a submitted paper immediately available to the reviewers so that it can be taken into account.

Having a standardized computational environment requires a concerted effort, or else it would descent into the ``XKCD 927'' problem \footnote{XKCD 927, How Standards proliferate: \url{https://xkcd.com/927/}}. Although we demonstrate our proposed computational environment works practically, academic groups such as ICA CM together with and a consortium of journal editors are in a better position to derive this standard.

\section{Coda: Trade-offs}

``Programmers know the benefits of everything and the trade-offs of nothing'', said Rich Hickey, the inventor of the Clojure language. We probably were too much in the \textit{programmer mode} in the above discussion and therefore we quote Hickey here as a self-criticism. It is important to remind ourselves once again, computational reproducibility is not wholly a technological issue, but in large part a social issue. When dealing with social issues, we must know trade-offs more so than in dealing with technological issues.

The above discussion focuses only on the availability of data and code, as well as the software used for executing the code. An issue that was completely ignored in the above discussion is computational resources. For example, we took weeks to check the code of Article M on an off-the-shelf notebook computer due to the exceptionally long running time. For Article J, the demo code for video feature extraction demands specific hardware and drivers. We happened to get access to both, but nevertheless failed to get that code run due to a missing script in the shared code---at least we were unable to find it. 

Our omission of discussing computational resources is intentional: we personally have inadequate resources to reproduce the most sophisticated approaches currently employed in the field---let alone running many of them in succession. But someone else might not face this issue. However, computational communication science will probably become more computational intensive in the future and it renders the task of computational reproducibility check like ours more demanding. Environmentally speaking, computational power should now be considered as a scarce resource due to the greenhouse gas emission associated with it \parencite[]{Wu:2022}. Reproducibility checks like ours could be perceived as wasteful, if one sees no value in such duplicated efforts---the \textit{re} in reproducibility.

Another omission in our discussion is that our suggestions increase the workload of computational communication researchers, whose time is already in scarcity. Getting reproducibility right is difficult, as seen in the code editing we needed to do even for articles we found to be reproducible. Although we have suggested automating this process as much as possible, there is still an upfront cost to learn about the tools and to write cleaner code---although we think these investments would produce considerable returns eventually. It circles back to the incentive question. Again, we have no solution.

Lastly, we want to acknowledge that it should not be expected that every (computational) communication scholar has the same level of computational expertise as the authors of the articles analyzed in this study. Even with computational methods' rising popularity, computational methods are only a part of the larger canon of methods used in the field. Our recommendations should thus not be understood as indispensable prerequisites to communication research in general, but as guidelines specifically for making computational communication research more reproducible. We believe that researchers who are building their computational expertise can benefit from code that has been written with reproducibility in mind, as it enables them to execute and inspect other's work without having to solve issues like outdated dependencies. Reproducibility should always be seen as an enabler of quality research, not a deterrent.

\section{Acknowledgments}

We would like to thank Shelby Wilcox (Michigan State University) for her help during the conceptualization of this study; Jun Sun and Arnim Bleier (both GESIS) for giving us advice on reproducing Python computational environments. 
\textit{Computational Communication Research} for fostering transparent and reproducible research. We also would like to thank the original authors who submitted their works to \textit{Computational Communication Research} and shared their code and data. Without their \textit{bona fides}, this analysis of computational reproducibility is not possible. Lastly, we want to thank the organizers of the ICA Hackathon 2023 in Toronto, where we met and worked on the foundation of this paper.
